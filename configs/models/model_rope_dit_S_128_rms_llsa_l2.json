{
    "_class_name": "RopeDiTTransformer2DModel",
    "num_attention_heads": 6,
    "attention_head_dim": 64,
    "num_layers": 12,
    "in_channels": 3,
    "out_channels": 3,
    "patch_size": 1,
    "sample_size": 128,
    "axes_dims_rope": [
        32,
        32
    ],
    "attn_type": "sparse_l2",
    "qk_norm": "rms_norm",
    "rope_size": 128,
    "token_permutation": true,
    "sparse_topk": 8
}